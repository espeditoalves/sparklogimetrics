{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **1. Introdução**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A regressão logística é um método popular para prever uma resposta categórica. É um caso especial de modelos Lineares Generalizados que prevê a probabilidade dos resultados. Em `spark.ml`, a regressão logística pode ser usada para prever um resultado binário usando regressão logística binomial ou pode ser usada para prever um resultado multiclasse usando regressão logística multinomial. Use o parâmetro `family` para selecionar entre esses dois algoritmos ou deixe-o indefinido e o Spark inferirá a variante correta.\n",
    "\n",
    "<font size=2>**Fonte:** [MLlib](https://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"https://miro.medium.com/max/1400/0*1KnKYuv0UDu_1-qM.gif?width=1191&height=670\" alt=\"Minha Figura\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.1 Carregando o pyspark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "H02jOAtLOpQY"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[*]').appName(\"Classificação com Spark 1.1.1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "YiVdw80-dQhs",
    "outputId": "88de605a-4e05-44b3-9fdf-70046a7d9e0b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://48912d32ce88:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Classificação com Spark 1.3</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f955c0c49d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **1.2 Carregando as principais funções**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "import sys # para julab, vscode não precisa\n",
    "sys.path.append('../../../') # para julab, vscode não precisa\n",
    "julab = '../../../'\n",
    "# from work.src.utils import *\n",
    "\n",
    "\n",
    "n = 'best_lr_model'\n",
    "caminho_modelo = 'work/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def calculate_auc_roc(df: DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calcula a AUC ROC para um DataFrame com colunas 'label' e 'prediction'.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame com as colunas 'label' e 'prediction'.\n",
    "\n",
    "    Returns:\n",
    "        float: Valor da AUC ROC.\n",
    "    \"\"\"\n",
    "    # Cria um avaliador para Binary Classification\n",
    "    evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "    \n",
    "    # Calcula a AUC ROC\n",
    "    auc_roc = evaluator.evaluate(df)\n",
    "    \n",
    "    return auc_roc\n",
    "\n",
    "def calculate_auc_pr(df: DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calcula a AUC PR para um DataFrame com colunas 'label' e 'prediction'.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame com as colunas 'label' e 'prediction'.\n",
    "\n",
    "    Returns:\n",
    "        float: Valor da AUC PR.\n",
    "    \"\"\"\n",
    "    # Cria um avaliador para Binary Classification\n",
    "    evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label\", metricName=\"areaUnderPR\")\n",
    "    \n",
    "    # Calcula a AUC PR\n",
    "    auc_pr = evaluator.evaluate(df)\n",
    "    \n",
    "    return auc_pr\n",
    "\n",
    "def calculate_ks(df: DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calcula o KS (Kolmogorov-Smirnov) para um DataFrame com colunas 'label' e 'prediction'.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): DataFrame com as colunas 'label' e 'prediction'.\n",
    "\n",
    "    Returns:\n",
    "        float: Valor do KS.\n",
    "    \"\"\"\n",
    "    # Ordenar o DataFrame pelas previsões\n",
    "    df_sorted = df.orderBy(F.col(\"prediction\").desc())\n",
    "    \n",
    "    # Calcular o número total de positivos e negativos\n",
    "    total_positives = df_sorted.filter(F.col(\"label\") == 1).count()\n",
    "    total_negatives = df_sorted.filter(F.col(\"label\") == 0).count()\n",
    "    \n",
    "    # Adicionar colunas de contagem acumulada\n",
    "    window_spec = Window.orderBy(F.col(\"prediction\").desc())\n",
    "    df_sorted = df_sorted.withColumn(\"cum_positives\", F.sum(F.when(F.col(\"label\") == 1, 1).otherwise(0)).over(window_spec))\n",
    "    df_sorted = df_sorted.withColumn(\"cum_negatives\", F.sum(F.when(F.col(\"label\") == 0, 1).otherwise(0)).over(window_spec))\n",
    "    \n",
    "    # Calcular taxas acumuladas\n",
    "    df_sorted = df_sorted.withColumn(\"tpr\", F.col(\"cum_positives\") / total_positives)\n",
    "    df_sorted = df_sorted.withColumn(\"fpr\", F.col(\"cum_negatives\") / total_negatives)\n",
    "    \n",
    "    # Calcular KS\n",
    "    df_sorted = df_sorted.withColumn(\"ks\", F.col(\"tpr\") - F.col(\"fpr\"))\n",
    "    ks_value = df_sorted.agg(F.max(col(\"ks\"))).collect()[0][0]\n",
    "    \n",
    "    return ks_value\n",
    "\n",
    "def calculate_confusion_matrix(df: DataFrame) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Calcula os valores de verdadeiro positivo (TP), verdadeiro negativo (TN),\n",
    "    falso positivo (FP) e falso negativo (FN) para um DataFrame com colunas 'label' e 'prediction'.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame com as colunas 'label' e 'prediction'.\n",
    "\n",
    "    Returns:\n",
    "        dict: Um dicionário com os valores de TP, TN, FP e FN.\n",
    "    \"\"\"\n",
    "    tp = df.filter((F.col('label') == 1) & (F.col('prediction') == 1)).count()\n",
    "    tn = df.filter((F.col('label') == 0) & (F.col('prediction') == 0)).count()\n",
    "    fp = df.filter((F.col('label') == 0) & (F.col('prediction') == 1)).count()\n",
    "    fn = df.filter((F.col('label') == 1) & (F.col('prediction') == 0)).count()\n",
    "    \n",
    "    return {'TP': tp, 'TN': tn, 'FP': fp, 'FN': fn}\n",
    "\n",
    "def calcula_mostra_matriz_confusao(df_transform_modelo: DataFrame, normalize: bool = False, percentage: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Calcula e exibe a matriz de confusão para um DataFrame com colunas 'label' e 'prediction'.\n",
    "\n",
    "    Args:\n",
    "        df_transform_modelo (DataFrame): DataFrame com as colunas 'label' e 'prediction'.\n",
    "        normalize (bool): Se True, normaliza os valores pela soma das linhas. Default é False.\n",
    "        percentage (bool): Se True, exibe os valores normalizados em percentual. Requer normalize=True. Default é True.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    tp = df_transform_modelo.select('label', 'prediction').where((F.col('label') == 1) & (F.col('prediction') == 1)).count()\n",
    "    tn = df_transform_modelo.select('label', 'prediction').where((F.col('label') == 0) & (F.col('prediction') == 0)).count()\n",
    "    fp = df_transform_modelo.select('label', 'prediction').where((F.col('label') == 0) & (F.col('prediction') == 1)).count()\n",
    "    fn = df_transform_modelo.select('label', 'prediction').where((F.col('label') == 1) & (F.col('prediction') == 0)).count()\n",
    "  \n",
    "    valorP = 1\n",
    "    valorN = 1\n",
    "\n",
    "    if normalize:\n",
    "        valorP = tp + fn\n",
    "        valorN = fp + tn\n",
    "  \n",
    "    if percentage and normalize:\n",
    "        valorP = valorP / 100\n",
    "        valorN = valorN / 100\n",
    "\n",
    "    print(' ' * 20, 'Previsto')\n",
    "    print(' ' * 15, 'Churn', ' ' * 5, 'Não-Churn')\n",
    "    print(' ' * 4, 'Churn', ' ' * 6, int(tp / valorP), ' ' * 7, int(fn / valorP))\n",
    "    print('Real')\n",
    "    print(' ' * 4, 'Não-Churn', ' ' * 2, int(fp / valorN), ' ' * 7, int(tn / valorN))\n",
    "\n",
    "\n",
    "def bootstrap_metric_spark(\n",
    "    data: DataFrame,\n",
    "    n_bootstrap: int = 100,\n",
    "    alpha: float = 0.95\n",
    ") -> Dict[str, Dict[str, List[float]]]:\n",
    "    \"\"\"\n",
    "    Calcula o intervalo de confiança e a média para várias métricas usando o método de bootstrap.\n",
    "\n",
    "    Args:\n",
    "    - data: DataFrame do Spark contendo os dados com as colunas 'label' e 'prediction'.\n",
    "    - n_bootstrap: Número de amostras bootstrap a serem geradas (padrão é 1000).\n",
    "    - alpha: Nível de confiança para o intervalo de confiança (padrão é 0.95).\n",
    "\n",
    "    Returns:\n",
    "    - Um dicionário onde as chaves são os nomes das métricas ('ks', 'auc', 'auc_pr') e os valores são dicionários contendo:\n",
    "      - 'scores': Lista de pontuações para a métrica.\n",
    "      - 'interval': Limites inferior e superior do intervalo de confiança.\n",
    "      - 'mean_score': Média das pontuações calculadas nas amostras bootstrap.\n",
    "      - 'std_dev': Desvio padrão das pontuações calculadas nas amostras bootstrap.\n",
    "    \"\"\"\n",
    "    bootstrapped_scores_ks = []\n",
    "    bootstrapped_scores_auc_roc = []\n",
    "    bootstrapped_scores_auc_pr = []\n",
    "    # Inicializa o gerador de números aleatórios para garantir reprodutibilidade\n",
    "    rng = np.random.RandomState(42)\n",
    "    print(f'Será realizada {n_bootstrap} iterações')\n",
    "    for i in range(n_bootstrap):\n",
    "        print(f'Execução iteração: {i}')\n",
    "        # Reamostragem com substituição\n",
    "        sample = data.sample(withReplacement=True, fraction=1.0, seed=rng.randint(1, 10000))\n",
    "        # `withReplacement` é True, cada linha do DataFrame pode ser escolhida mais de uma vez na amostra.\n",
    "        # `fraction=1.0` Um valor de 1.0 significa que a amostra deve ter o mesmo número de linhas que o DataFrame original,\n",
    "        # se fosse 0.5, a amostra teria aproximadamente 50% das linhas do DataFrame original.\n",
    "        # seed=rng.randint(1, 10000): Usando a abordagem com rng, você pode obter uma nova semente aleatória para cada iteração.\n",
    "\n",
    "        print(f'Sample count: {sample.count()}')\n",
    "        \n",
    "        # Cálculo da métrica\n",
    "        score_ks = calculate_ks(sample)\n",
    "        score_auc_roc = calculate_auc_roc(sample)\n",
    "        score_auc_pr = calculate_auc_pr(sample)\n",
    "        \n",
    "        bootstrapped_scores_ks.append(score_ks)\n",
    "        bootstrapped_scores_auc_roc.append(score_auc_roc)\n",
    "        bootstrapped_scores_auc_pr.append(score_auc_pr)\n",
    "        print('---'*5)\n",
    "    \n",
    "    # Lista contendo as listas de pontuações e suas respectivas chaves\n",
    "    listas = [\n",
    "        ('ks', bootstrapped_scores_ks),\n",
    "        ('auc', bootstrapped_scores_auc_roc),\n",
    "        ('auc_pr', bootstrapped_scores_auc_pr)\n",
    "    ]\n",
    "    resultados = {}\n",
    "    resultados_scores = {}\n",
    "    # Iterar sobre cada lista e calcular os valores desejados\n",
    "    for chave, scores in listas:\n",
    "        sorted_scores = np.array(scores)\n",
    "        lower_bound = float(np.percentile(sorted_scores, (1 - alpha) / 2 * 100))\n",
    "        upper_bound = float(np.percentile(sorted_scores, (1 + alpha) / 2 * 100))\n",
    "        mean_score = float(np.mean(sorted_scores))\n",
    "        std_dev = float(np.std(sorted_scores, ddof=1))  # Usando ddof=1 para amostras\n",
    "\n",
    "        confidence_interval = [lower_bound, upper_bound]\n",
    "    \n",
    "        resultados[chave] = {\n",
    "            'confidence_interval': confidence_interval,\n",
    "            'mean_score': mean_score,\n",
    "            'std_dev': std_dev\n",
    "        }\n",
    "\n",
    "        resultados_scores[chave] = {\n",
    "            'scores': scores,\n",
    "        }\n",
    "    return resultados_scores, resultados\n",
    "\n",
    "\n",
    "def df_scores(scores_dic: Dict[str, Dict[str, List[float]]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converte um dicionário de scores em um DataFrame do Pandas.\n",
    "\n",
    "    Args:\n",
    "        scores_dic (dict): Dicionário contendo os scores. O formato esperado é:\n",
    "            {\n",
    "                'ks': {'scores': list},\n",
    "                'auc': {'scores': list},\n",
    "                'auc_pr': {'scores': list}\n",
    "            }\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame contendo as listas de scores com as seguintes colunas:\n",
    "            - 'ks.scores': Scores de KS.\n",
    "            - 'auc.scores': Scores de AUC.\n",
    "            - 'auc_pr.scores': Scores de AUC-PR.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    df['ks.scores'] = scores_dic['ks']['scores']\n",
    "    df['auc.scores'] = scores_dic['auc']['scores']\n",
    "    df['auc_pr.scores'] = scores_dic['auc_pr']['scores']\n",
    "    return df\n",
    "\n",
    "### PERMUTACION TESTE\n",
    "\n",
    "def permutation_test(\n",
    "    array1: List[float],\n",
    "    array2: List[float],\n",
    "    anscreen: bool = False,\n",
    "    alpha: float = 0.05\n",
    ") -> Tuple[float, List[float], float, List[str]]:\n",
    "    \"\"\"\n",
    "    Realiza um teste de permutação para comparar as médias de dois arrays.\n",
    "\n",
    "    Args:\n",
    "        array1 (List[float]): O primeiro array de dados.\n",
    "        array2 (List[float]): O segundo array de dados.\n",
    "        anscreen (bool): Se True, imprime os resultados na tela. Default é False.\n",
    "        alpha (float): Nível de significância para o teste (p-valor). Default é 0.05.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, List[float], float, List[str]]:\n",
    "            - p_val (float): Valor p do teste de permutação.\n",
    "            - mean_lst (List[float]): Lista das diferenças médias permutadas.\n",
    "            - mean_diff (float): Diferença média observada entre os dois arrays.\n",
    "            - text_lst (List[str]): Lista de mensagens interpretativas sobre o teste.\n",
    "    \"\"\"\n",
    "    # Garantindo a entrada com numpy array\n",
    "    array1 = np.array(array1)\n",
    "    array2 = np.array(array2)\n",
    "    \n",
    "    # Cálculo das médias de cada vetor\n",
    "    avg_array1 = array1.mean()\n",
    "    avg_array2 = array2.mean()\n",
    "    \n",
    "    # Diferença entre as médias\n",
    "    mean_diff = avg_array1 - avg_array2\n",
    "    full_array = np.concatenate([array1, array2])\n",
    "    mean_lst = []\n",
    "    # Defina a semente aleatória para reprodutibilidade\n",
    "    np.random.seed(42)\n",
    "    for _ in range(10000):\n",
    "        # Com reposição: bootstrapping\n",
    "        avg1 = np.random.choice(full_array, size=len(array1), replace=True).mean()\n",
    "        avg2 = np.random.choice(full_array, size=len(array2), replace=True).mean()\n",
    "        # reprece = True, Assume que qualquer valor pode vir de uma das duas listas, convergÊncia para Normal.\n",
    "        mean_lst.append(avg1 - avg2)\n",
    "    \n",
    "    if mean_diff > 0:\n",
    "        # p_val = np.sum(np.array(mean_lst) > mean_diff) / 1\n",
    "        p_val = np.sum(np.array(mean_lst) > mean_diff) / len(mean_lst)\n",
    "    else:\n",
    "        # p_val = np.sum(np.array(mean_lst) < mean_diff) / 1\n",
    "        p_val = np.sum(np.array(mean_lst) < mean_diff) / len(mean_lst)\n",
    "    \n",
    "    text_lst = [\"\\n Teste de Significancia \", \n",
    "                \"**$H_0$:** Diferença entre as médias das métricas é zero. \\n\",\n",
    "                f\" Arrays sizes: {len(array1)}, {len(array2)} \",\n",
    "                \"* Difference between averages: %.4f - %.4f = %.4f\" % (avg_array1, avg_array2, mean_diff),\n",
    "                \"* p_val = %.4f \" %p_val]\n",
    "    \n",
    "    if p_val > alpha:\n",
    "        text_lst.append(f'The model seems to produce similar results with CI-{1 - alpha} (fail to reject H0).\\n')\n",
    "    else:\n",
    "        text_lst.append(f'The model seems to produce different results with CI-{1 - alpha} (reject H0).\\n')\n",
    "    \n",
    "    if anscreen:\n",
    "        for line in text_lst:\n",
    "            print(line)   \n",
    "    return p_val, mean_lst, mean_diff, text_lst\n",
    "\n",
    "def bootstrap_metric_spark_permutacion(\n",
    "    data1: DataFrame,\n",
    "    data2: DataFrame,\n",
    "    n_bootstrap: int = 100,\n",
    "    alpha: float = 0.95\n",
    ") -> Tuple[Dict[str, Dict[str, List[float]]], Dict[str, Dict[str, float]]]:\n",
    "    \"\"\"\n",
    "    Calcula o intervalo de confiança e a média para várias métricas usando o método de bootstrap e realiza um teste de permutação para comparar as métricas entre dois DataFrames.\n",
    "\n",
    "    Args:\n",
    "        data1 (DataFrame): Primeiro DataFrame do Spark contendo os dados com as colunas 'label' e 'prediction'.\n",
    "        data2 (DataFrame): Segundo DataFrame do Spark contendo os dados com as colunas 'label' e 'prediction'.\n",
    "        n_bootstrap (int): Número de amostras bootstrap a serem geradas. Default é 100.\n",
    "        alpha (float): Nível de confiança para o intervalo de confiança. Default é 0.95.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[str, Dict[str, List[float]]], Dict[str, Dict[str, float]]]:\n",
    "            - resultados_scores (Dict[str, Dict[str, List[float]]]): Dicionário com os scores bootstrap para cada métrica.\n",
    "            - resultados (Dict[str, Dict[str, float]]): Dicionário com os intervalos de confiança, médias e desvios padrão das métricas.\n",
    "            - p_values (Dict[str, float]): Dicionário com os valores p dos testes de permutação para cada métrica.\n",
    "    \"\"\"\n",
    "    def calculate_metrics(data: DataFrame) -> Dict[str, float]:\n",
    "        \"\"\"Calcula as métricas de desempenho para um DataFrame.\"\"\"\n",
    "        return {\n",
    "            'ks': calculate_ks(data),\n",
    "            'auc': calculate_auc_roc(data),\n",
    "            'auc_pr': calculate_auc_pr(data)\n",
    "        }\n",
    "\n",
    "    bootstrapped_scores1 = {metric: [] for metric in ['ks', 'auc', 'auc_pr']}\n",
    "    bootstrapped_scores2 = {metric: [] for metric in ['ks', 'auc', 'auc_pr']}\n",
    "    \n",
    "    rng = np.random.RandomState(42)\n",
    "    rng2 = np.random.RandomState(42)\n",
    "    # está gerando várias amostras bootstrap a partir de data1 e data2\n",
    "    for i in range(n_bootstrap):\n",
    "        print(f'Iteração {i+1}/{n_bootstrap}')\n",
    "        #withReplacement=True: Indica que a amostragem é com reposição (Bootstrapping)\n",
    "        sample1 = data1.sample(withReplacement=True, fraction=1.0, seed=rng.randint(1, 10000))\n",
    "        sample2 = data2.sample(withReplacement=True, fraction=1.0, seed=rng2.randint(1, 10000))\n",
    "        print(f'Sample1 count: {sample1.count()}')\n",
    "        print(f'Sample2 count: {sample2.count()}')\n",
    "        print('--'*5)\n",
    "        \n",
    "        metrics1 = calculate_metrics(sample1)\n",
    "        metrics2 = calculate_metrics(sample2)\n",
    "        \n",
    "        for metric in ['ks', 'auc', 'auc_pr']:\n",
    "            bootstrapped_scores1[metric].append(metrics1[metric])\n",
    "            bootstrapped_scores2[metric].append(metrics2[metric])\n",
    "    \n",
    "  \n",
    "    results = {}\n",
    "    results_scores_permutacion = {}\n",
    "    \n",
    "    for metric in ['ks', 'auc', 'auc_pr']:\n",
    "        scores1 = np.array(bootstrapped_scores1[metric])\n",
    "        scores2 = np.array(bootstrapped_scores2[metric])\n",
    "        \n",
    "        # Verificar a contrução do intervalo de Confiança\n",
    "        lower_bound1 = float(np.percentile(scores1, (1 - alpha) / 2 * 100))\n",
    "        upper_bound1 = float(np.percentile(scores1, (1 + alpha) / 2 * 100))\n",
    "        mean_score1 = float(np.mean(scores1))\n",
    "        std_dev1 = float(np.std(scores1, ddof=1))\n",
    "        \n",
    "        lower_bound2 = float(np.percentile(scores2, (1 - alpha) / 2 * 100))\n",
    "        upper_bound2 = float(np.percentile(scores2, (1 + alpha) / 2 * 100))\n",
    "        mean_score2 = float(np.mean(scores2))\n",
    "        std_dev2 = float(np.std(scores2, ddof=1))\n",
    "        \n",
    "        results[metric] = {\n",
    "            'confidence_interval1': [lower_bound1, upper_bound1],\n",
    "            'mean_score1': mean_score1,\n",
    "            'std_dev1': std_dev1,\n",
    "            'confidence_interval2': [lower_bound2, upper_bound2],\n",
    "            'mean_score2': mean_score2,\n",
    "            'std_dev2': std_dev2\n",
    "        }\n",
    "        \n",
    "        # Teste de permutação entre os dois conjuntos de scores\n",
    "        # Cada elemento dos scores, foi gerado por amostras com reposição (Bootstrapping )\n",
    "        p_val, mean_lst, mean_diff, text_lst = permutation_test(scores1.tolist(), scores2.tolist())\n",
    "        print('####'*10)\n",
    "        print(metric)\n",
    "        print('---'*10)\n",
    "        print(text_lst)\n",
    "        \n",
    "        results_scores_permutacion[metric] = {\n",
    "            'scores1': scores1.tolist(),\n",
    "            'scores2': scores2.tolist(),\n",
    "            'p_value': p_val,\n",
    "            'mean_diff': mean_diff\n",
    "        }\n",
    "    \n",
    "    return results_scores_permutacion, results\n",
    "\n",
    "def df_scores_1_2(scores_dic: Dict[str, Dict[str, List[float]]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converte um dicionário de scores em um DataFrame do Pandas.\n",
    "\n",
    "    Args:\n",
    "        scores_dic (dict): Dicionário contendo os scores1 e scores2.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame contendo as listas de scores e informações de teste com as seguintes colunas:\n",
    "            - 'ks.scores1': Scores de KS para o primeiro conjunto.\n",
    "            - 'ks.scores2': Scores de KS para o segundo conjunto.\n",
    "            - 'auc.scores1': Scores de AUC para o primeiro conjunto.\n",
    "            - 'auc.scores2': Scores de AUC para o segundo conjunto.\n",
    "            - 'auc_pr.scores1': Scores de AUC-PR para o primeiro conjunto.\n",
    "            - 'auc_pr.scores2': Scores de AUC-PR para o segundo conjunto.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    df['ks.scores1'] = scores_dic['ks']['scores1']\n",
    "    df['ks.scores2'] = scores_dic['ks']['scores2']\n",
    "    print(f\"KS p_value': {scores_dic['ks']['p_value']}\")\n",
    "    print(f\"KS mean_diff': {scores_dic['ks']['mean_diff']}\")\n",
    "    \n",
    "    df['auc.scores1'] = scores_dic['auc']['scores1']\n",
    "    df['auc.scores2'] = scores_dic['auc']['scores2']\n",
    "    print(f\"auc p_value: {scores_dic['auc']['p_value']}\")\n",
    "    print(f\"auc mean_diff: {scores_dic['auc']['mean_diff']}\")\n",
    "    \n",
    "    df['auc_pr.scores1'] = scores_dic['auc_pr']['scores1']\n",
    "    df['auc_pr.scores2'] = scores_dic['auc_pr']['scores2']\n",
    "    print(f\"auc_pr p_value: {scores_dic['auc_pr']['p_value']}\")\n",
    "    print(f\"auc_pr mean_diff: {scores_dic['auc_pr']['mean_diff']}\")\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bootstrap = 5\n",
    "## A diferença é :\n",
    "\n",
    "    # if mean_diff > 0:\n",
    "    #     # p_val = np.sum(np.array(mean_lst) > mean_diff) / 1\n",
    "    #     p_val = np.sum(np.array(mean_lst) > mean_diff) / len(mean_lst)\n",
    "    # else:\n",
    "    #     # p_val = np.sum(np.array(mean_lst) < mean_diff) / 1\n",
    "    #     p_val = np.sum(np.array(mean_lst) < mean_diff) / len(mean_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZHCjR2twqLt",
    "tags": []
   },
   "source": [
    "## **2.3 Métricas BASE DE VALIDAÇÃO**\n",
    "\n",
    "<font size=2>**Documentação:**</font>\n",
    "<font size=2>[LogisticRegressionTrainingSummary](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.LogisticRegressionTrainingSummary.html)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions_val = spark.read.orc(f'{julab}work/data/final/predictions_val_{n}.orc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PmIsEpfg0gj2",
    "outputId": "6a52e918-84b8-4001-a86d-1c4bac772eae",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predictions_val.show(truncate=False)\n",
    "# +-----------------------------------------------------------------------------------------------------------+-----+------------------------------------------+----------------------------------------+----------+\n",
    "# |features                                                                                                   |label|rawPrediction                             |probability                             |prediction|\n",
    "# +-----------------------------------------------------------------------------------------------------------+-----+------------------------------------------+----------------------------------------+----------+\n",
    "# |(24,[1,2,5,6,10,11,12,13,14,18,23],[12.0,75.85,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                       |0    |[0.05103567311346513,-0.05103567311346513]|[0.5127561496338285,0.48724385036617146]|0.0       |\n",
    "# |(24,[1,2,3,5,8,12,13,14,19,21],[69.0,61.45,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                               |0    |[2.382789551862372,-2.382789551862372]    |[0.9155054708840569,0.08449452911594313]|0.0       |\n",
    "# |(24,[1,2,3,5,12,13,15,17,22],[46.0,80.8824189403559,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                          |1    |[-1.1704365714764247,1.1704365714764247]  |[0.23677608099237907,0.7632239190076209]|1.0       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICAS RESUMO DA BASE DE VALIDAÇÃO\n",
      "AUC ROC: 0.7897105323794532\n",
      "AUC PR: 0.7617664882695159\n",
      "KS: 0.5794210647589065\n"
     ]
    }
   ],
   "source": [
    "print('METRICAS RESUMO DA BASE DE VALIDAÇÃO')\n",
    "\n",
    "auc_roc = calculate_auc_roc(predictions_val)\n",
    "print(f\"AUC ROC: {auc_roc}\")\n",
    "auc_pr = calculate_auc_pr(predictions_val)\n",
    "print(f\"AUC PR: {auc_pr}\")\n",
    "ks = calculate_ks(predictions_val)\n",
    "print(f\"KS: {ks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NVSZDqgWIOdr",
    "outputId": "8bb57e0d-46a4-4481-c0b4-b3fb5a02119d",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TP': 870, 'TN': 774, 'FP': 235, 'FN': 201}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_confusion_matrix(predictions_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09sQOK-xLwkS",
    "outputId": "2eebdcd8-7cae-4d15-af12-d3807c47edd0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Previsto\n",
      "                Churn       Não-Churn\n",
      "     Churn        870         201\n",
      "Real\n",
      "     Não-Churn    235         774\n"
     ]
    }
   ],
   "source": [
    "calcula_mostra_matriz_confusao(predictions_val, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **2.4 Métricas BASE DE TESTE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions_test = spark.read.orc(f'{julab}work/data/final/predictions_test_{n}.orc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICAS RESUMO DA BASE DE TESTE\n",
      "AUC ROC: 0.7630023396167441\n",
      "AUC PR: 0.7168174012626157\n",
      "KS: 0.5260046792334883\n"
     ]
    }
   ],
   "source": [
    "print('METRICAS RESUMO DA BASE DE TESTE')\n",
    "\n",
    "auc_roc = calculate_auc_roc(predictions_test)\n",
    "print(f\"AUC ROC: {auc_roc}\")\n",
    "auc_pr = calculate_auc_pr(predictions_test)\n",
    "print(f\"AUC PR: {auc_pr}\")\n",
    "ks = calculate_ks(predictions_test)\n",
    "print(f\"KS: {ks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TP': 830, 'TN': 761, 'FP': 290, 'FN': 205}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_confusion_matrix(predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Previsto\n",
      "                Churn       Não-Churn\n",
      "     Churn        830         205\n",
      "Real\n",
      "     Não-Churn    290         761\n"
     ]
    }
   ],
   "source": [
    "calcula_mostra_matriz_confusao(predictions_test, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **3. Boostramp, intervalos de confiança, Permutacion test**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **3.1 Boostramp - VALIDAÇÃO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2080"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_val.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Será realizada 5 iterações\n",
      "Execução iteração: 0\n",
      "Sample count: 2129\n",
      "---------------\n",
      "Execução iteração: 1\n",
      "Sample count: 2060\n",
      "---------------\n",
      "Execução iteração: 2\n",
      "Sample count: 2062\n",
      "---------------\n",
      "Execução iteração: 3\n",
      "Sample count: 2069\n",
      "---------------\n",
      "Execução iteração: 4\n",
      "Sample count: 2091\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# Calcule intervalos de confiança e média\n",
    "scores, resultados = bootstrap_metric_spark(data = predictions_val, n_bootstrap = n_bootstrap)\n",
    "\n",
    "# Será realizada 5 iterações\n",
    "# Execução iteração: 0\n",
    "# Sample count: 2129\n",
    "# ---------------\n",
    "# Execução iteração: 1\n",
    "# Sample count: 2060\n",
    "# ---------------\n",
    "# Execução iteração: 2\n",
    "# Sample count: 2062\n",
    "# ---------------\n",
    "# Execução iteração: 3\n",
    "# Sample count: 2069\n",
    "# ---------------\n",
    "# Execução iteração: 4\n",
    "# Sample count: 2091\n",
    "# ---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>confidence_interval</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>std_dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ks</th>\n",
       "      <td>[0.5587163142695343, 0.5940801326675024]</td>\n",
       "      <td>0.579402</td>\n",
       "      <td>0.014293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc</th>\n",
       "      <td>[0.7793581571347672, 0.7970400663337511]</td>\n",
       "      <td>0.789701</td>\n",
       "      <td>0.007146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_pr</th>\n",
       "      <td>[0.7587768523408239, 0.7755809192338347]</td>\n",
       "      <td>0.765438</td>\n",
       "      <td>0.007362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             confidence_interval mean_score   std_dev\n",
       "ks      [0.5587163142695343, 0.5940801326675024]   0.579402  0.014293\n",
       "auc     [0.7793581571347672, 0.7970400663337511]   0.789701  0.007146\n",
       "auc_pr  [0.7587768523408239, 0.7755809192338347]   0.765438  0.007362"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(resultados).T\n",
    "# \tconfidence_interval\tmean_score\tstd_dev\n",
    "# ks\t[0.5587163142695343, 0.5940801326675024]\t0.579402\t0.014293\n",
    "# auc\t[0.7793581571347672, 0.7970400663337511]\t0.789701\t0.007146\n",
    "# auc_pr\t[0.7587768523408239, 0.7755809192338347]\t0.765438\t0.007362"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ks.scores</th>\n",
       "      <th>auc.scores</th>\n",
       "      <th>auc_pr.scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.583983</td>\n",
       "      <td>0.791992</td>\n",
       "      <td>0.769682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.576596</td>\n",
       "      <td>0.788298</td>\n",
       "      <td>0.758596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.556730</td>\n",
       "      <td>0.778365</td>\n",
       "      <td>0.762273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.595138</td>\n",
       "      <td>0.797569</td>\n",
       "      <td>0.776236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.584564</td>\n",
       "      <td>0.792282</td>\n",
       "      <td>0.760402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ks.scores  auc.scores  auc_pr.scores\n",
       "0   0.583983    0.791992       0.769682\n",
       "1   0.576596    0.788298       0.758596\n",
       "2   0.556730    0.778365       0.762273\n",
       "3   0.595138    0.797569       0.776236\n",
       "4   0.584564    0.792282       0.760402"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df = df_scores(scores)\n",
    "scores_df\n",
    "\n",
    "# ks.scores\tauc.scores\tauc_pr.scores\n",
    "# 0\t0.583983\t0.791992\t0.769682\n",
    "# 1\t0.576596\t0.788298\t0.758596\n",
    "# 2\t0.556730\t0.778365\t0.762273\n",
    "# 3\t0.595138\t0.797569\t0.776236\n",
    "# 4\t0.584564\t0.792282\t0.760402"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **3.2 Boostramp e Permutacion test**\n",
    "\n",
    "* BASE: VALIDAÇÃO\n",
    "* BASE: TESTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2080\n",
      "2086\n"
     ]
    }
   ],
   "source": [
    "print(predictions_val.count())\n",
    "print(predictions_test.count())\n",
    "# 2080\n",
    "# 2086"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteração 1/5\n",
      "Sample1 count: 2129\n",
      "Sample2 count: 2136\n",
      "----------\n",
      "Iteração 2/5\n",
      "Sample1 count: 2060\n",
      "Sample2 count: 2066\n",
      "----------\n",
      "Iteração 3/5\n",
      "Sample1 count: 2062\n",
      "Sample2 count: 2066\n",
      "----------\n",
      "Iteração 4/5\n",
      "Sample1 count: 2069\n",
      "Sample2 count: 2075\n",
      "----------\n",
      "Iteração 5/5\n",
      "Sample1 count: 2091\n",
      "Sample2 count: 2095\n",
      "----------\n",
      "########################################\n",
      "ks\n",
      "------------------------------\n",
      "['\\n Teste de Significancia ', '**$H_0$:** Diferença entre as médias das métricas é zero. \\n', ' Arrays sizes: 5, 5 ', '* Difference between averages: 0.5794 - 0.5276 = 0.0518', '* p_val = 0.0049 ', 'The model seems to produce different results with CI-0.95 (reject H0).\\n']\n",
      "########################################\n",
      "auc\n",
      "------------------------------\n",
      "['\\n Teste de Significancia ', '**$H_0$:** Diferença entre as médias das métricas é zero. \\n', ' Arrays sizes: 5, 5 ', '* Difference between averages: 0.7897 - 0.7638 = 0.0259', '* p_val = 0.0049 ', 'The model seems to produce different results with CI-0.95 (reject H0).\\n']\n",
      "########################################\n",
      "auc_pr\n",
      "------------------------------\n",
      "['\\n Teste de Significancia ', '**$H_0$:** Diferença entre as médias das métricas é zero. \\n', ' Arrays sizes: 5, 5 ', '* Difference between averages: 0.7654 - 0.7160 = 0.0494', '* p_val = 0.0018 ', 'The model seems to produce different results with CI-0.95 (reject H0).\\n']\n"
     ]
    }
   ],
   "source": [
    "resultados_1_2_permutacion, resultados_1_2 = bootstrap_metric_spark_permutacion(data1 = predictions_val, data2 = predictions_test, n_bootstrap = n_bootstrap)\n",
    "# Iteração 1/5\n",
    "# Sample1 count: 2129\n",
    "# Sample2 count: 2136\n",
    "# ----------\n",
    "# Iteração 2/5\n",
    "# Sample1 count: 2060\n",
    "# Sample2 count: 2066\n",
    "# ----------\n",
    "# Iteração 3/5\n",
    "# Sample1 count: 2062\n",
    "# Sample2 count: 2066\n",
    "# ----------\n",
    "# Iteração 4/5\n",
    "# Sample1 count: 2069\n",
    "# Sample2 count: 2075\n",
    "# ----------\n",
    "# Iteração 5/5\n",
    "# Sample1 count: 2091\n",
    "# Sample2 count: 2095\n",
    "# ----------\n",
    "# ########################################\n",
    "# ks\n",
    "# ------------------------------\n",
    "# ['\\n Teste de Significancia ', '**$H_0$:** Diferença entre as médias das métricas é zero. \\n', ' Arrays sizes: 5, 5 ', '* Difference between averages: 0.5794 - 0.5276 = 0.0518', '* p_val = 49.0000 ', 'The model seems to produce similar results with CI-0.95 (fail to reject H0).\\n']\n",
    "# ########################################\n",
    "# auc\n",
    "# ------------------------------\n",
    "# ['\\n Teste de Significancia ', '**$H_0$:** Diferença entre as médias das métricas é zero. \\n', ' Arrays sizes: 5, 5 ', '* Difference between averages: 0.7897 - 0.7638 = 0.0259', '* p_val = 49.0000 ', 'The model seems to produce similar results with CI-0.95 (fail to reject H0).\\n']\n",
    "# ########################################\n",
    "# auc_pr\n",
    "# ------------------------------\n",
    "# ['\\n Teste de Significancia ', '**$H_0$:** Diferença entre as médias das métricas é zero. \\n', ' Arrays sizes: 5, 5 ', '* Difference between averages: 0.7654 - 0.7160 = 0.0494', '* p_val = 18.0000 ', 'The model seems to produce similar results with CI-0.95 (fail to reject H0).\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>confidence_interval1</th>\n",
       "      <th>mean_score1</th>\n",
       "      <th>std_dev1</th>\n",
       "      <th>confidence_interval2</th>\n",
       "      <th>mean_score2</th>\n",
       "      <th>std_dev2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ks</th>\n",
       "      <td>[0.5587163142695343, 0.5940801326675024]</td>\n",
       "      <td>0.579402</td>\n",
       "      <td>0.014293</td>\n",
       "      <td>[0.4926559689045311, 0.5510315509616689]</td>\n",
       "      <td>0.527572</td>\n",
       "      <td>0.02525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc</th>\n",
       "      <td>[0.7793581571347672, 0.7970400663337511]</td>\n",
       "      <td>0.789701</td>\n",
       "      <td>0.007146</td>\n",
       "      <td>[0.7463279844522657, 0.7755157754808344]</td>\n",
       "      <td>0.763786</td>\n",
       "      <td>0.012625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_pr</th>\n",
       "      <td>[0.7587768523408239, 0.7755809192338347]</td>\n",
       "      <td>0.765438</td>\n",
       "      <td>0.007362</td>\n",
       "      <td>[0.6988262164456484, 0.7300824389636984]</td>\n",
       "      <td>0.716001</td>\n",
       "      <td>0.01349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            confidence_interval1 mean_score1  std_dev1  \\\n",
       "ks      [0.5587163142695343, 0.5940801326675024]    0.579402  0.014293   \n",
       "auc     [0.7793581571347672, 0.7970400663337511]    0.789701  0.007146   \n",
       "auc_pr  [0.7587768523408239, 0.7755809192338347]    0.765438  0.007362   \n",
       "\n",
       "                            confidence_interval2 mean_score2  std_dev2  \n",
       "ks      [0.4926559689045311, 0.5510315509616689]    0.527572   0.02525  \n",
       "auc     [0.7463279844522657, 0.7755157754808344]    0.763786  0.012625  \n",
       "auc_pr  [0.6988262164456484, 0.7300824389636984]    0.716001   0.01349  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(resultados_1_2).T\n",
    "# \tconfidence_interval1\tmean_score1\tstd_dev1\tconfidence_interval2\tmean_score2\tstd_dev2\n",
    "# ks\t[0.5587163142695343, 0.5940801326675024]\t0.579402\t0.014293\t[0.4926559689045311, 0.5510315509616689]\t0.527572\t0.02525\n",
    "# auc\t[0.7793581571347672, 0.7970400663337511]\t0.789701\t0.007146\t[0.7463279844522657, 0.7755157754808344]\t0.763786\t0.012625\n",
    "# auc_pr\t[0.7587768523408239, 0.7755809192338347]\t0.765438\t0.007362\t[0.6988262164456484, 0.7300824389636984]\t0.716001\t0.01349"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KS p_value': 0.0049\n",
      "KS mean_diff': 0.05183035078404763\n",
      "auc p_value: 0.0049\n",
      "auc mean_diff: 0.025915175392023593\n",
      "auc_pr p_value: 0.0018\n",
      "auc_pr mean_diff: 0.04943703014541301\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ks.scores1</th>\n",
       "      <th>ks.scores2</th>\n",
       "      <th>auc.scores1</th>\n",
       "      <th>auc.scores2</th>\n",
       "      <th>auc_pr.scores1</th>\n",
       "      <th>auc_pr.scores2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.583983</td>\n",
       "      <td>0.490242</td>\n",
       "      <td>0.791992</td>\n",
       "      <td>0.745121</td>\n",
       "      <td>0.769682</td>\n",
       "      <td>0.706142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.576596</td>\n",
       "      <td>0.551584</td>\n",
       "      <td>0.788298</td>\n",
       "      <td>0.775792</td>\n",
       "      <td>0.758596</td>\n",
       "      <td>0.723848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.556730</td>\n",
       "      <td>0.514383</td>\n",
       "      <td>0.778365</td>\n",
       "      <td>0.757191</td>\n",
       "      <td>0.762273</td>\n",
       "      <td>0.721227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.595138</td>\n",
       "      <td>0.535587</td>\n",
       "      <td>0.797569</td>\n",
       "      <td>0.767793</td>\n",
       "      <td>0.776236</td>\n",
       "      <td>0.730775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.584564</td>\n",
       "      <td>0.546064</td>\n",
       "      <td>0.792282</td>\n",
       "      <td>0.773032</td>\n",
       "      <td>0.760402</td>\n",
       "      <td>0.698013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ks.scores1  ks.scores2  auc.scores1  auc.scores2  auc_pr.scores1  \\\n",
       "0    0.583983    0.490242     0.791992     0.745121        0.769682   \n",
       "1    0.576596    0.551584     0.788298     0.775792        0.758596   \n",
       "2    0.556730    0.514383     0.778365     0.757191        0.762273   \n",
       "3    0.595138    0.535587     0.797569     0.767793        0.776236   \n",
       "4    0.584564    0.546064     0.792282     0.773032        0.760402   \n",
       "\n",
       "   auc_pr.scores2  \n",
       "0        0.706142  \n",
       "1        0.723848  \n",
       "2        0.721227  \n",
       "3        0.730775  \n",
       "4        0.698013  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scores_result = df_scores_1_2(resultados_1_2_permutacion)\n",
    "df_scores_result\n",
    "# KS p_value': 49.0\n",
    "# KS mean_diff': 0.05183035078404763\n",
    "# auc p_value: 49.0\n",
    "# auc mean_diff: 0.025915175392023593\n",
    "# auc_pr p_value: 18.0\n",
    "# auc_pr mean_diff: 0.04943703014541301\n",
    "# ks.scores1\tks.scores2\tauc.scores1\tauc.scores2\tauc_pr.scores1\tauc_pr.scores2\n",
    "# 0\t0.583983\t0.490242\t0.791992\t0.745121\t0.769682\t0.706142\n",
    "# 1\t0.576596\t0.551584\t0.788298\t0.775792\t0.758596\t0.723848\n",
    "# 2\t0.556730\t0.514383\t0.778365\t0.757191\t0.762273\t0.721227\n",
    "# 3\t0.595138\t0.535587\t0.797569\t0.767793\t0.776236\t0.730775\n",
    "# 4\t0.584564\t0.546064\t0.792282\t0.773032\t0.760402\t0.698013\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **4. Tempo de execução**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de execução: 0:01:19.614344\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"Tempo de execução: {execution_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "BsjP6Rz-sRkn"
   ],
   "name": "projeto.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (Poetry)",
   "language": "python",
   "name": "image-spark-project-py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
